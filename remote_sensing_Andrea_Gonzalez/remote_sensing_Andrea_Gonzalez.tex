%  LaTeX support: latex@mdpi.com 
%  For support, please attach all files needed for compiling as well as the log file, and specify your operating system, LaTeX version, and LaTeX editor.

%=================================================================
\documentclass[journal,article,submit,pdftex,moreauthors]{Definitions/mdpi} 
\usepackage{mathrsfs}
\usepackage{makecell}
\usepackage{multicol}
\usepackage{subcaption}
\usepackage{caption}
\captionsetup[subfigure]{justification=centering}
\usepackage{float} %figure inside minipage


%--------------------
% Class Options:
%--------------------
%----------
% journal
%----------
% Choose between the following MDPI journals:
% acoustics, actuators, addictions, admsci, adolescents, aerobiology, aerospace, agriculture, agriengineering, agrochemicals, agronomy, ai, air, algorithms, allergies, alloys, analytica, analytics, anatomia, animals, antibiotics, antibodies, antioxidants, applbiosci, appliedchem, appliedmath, applmech, applmicrobiol, applnano, applsci, aquacj, architecture, arm, arthropoda, arts, asc, asi, astronomy, atmosphere, atoms, audiolres, automation, axioms, bacteria, batteries, bdcc, behavsci, beverages, biochem, bioengineering, biologics, biology, biomass, biomechanics, biomed, biomedicines, biomedinformatics, biomimetics, biomolecules, biophysica, biosensors, biotech, birds, bloods, blsf, brainsci, breath, buildings, businesses, cancers, carbon, cardiogenetics, catalysts, cells, ceramics, challenges, chemengineering, chemistry, chemosensors, chemproc, children, chips, cimb, civileng, cleantechnol, climate, clinpract, clockssleep, cmd, coasts, coatings, colloids, colorants, commodities, compounds, computation, computers, condensedmatter, conservation, constrmater, cosmetics, covid, crops, cryptography, crystals, csmf, ctn, curroncol, cyber, dairy, data, ddc, dentistry, dermato, dermatopathology, designs, devices, diabetology, diagnostics, dietetics, digital, disabilities, diseases, diversity, dna, drones, dynamics, earth, ebj, ecologies, econometrics, economies, education, ejihpe, electricity, electrochem, electronicmat, electronics, encyclopedia, endocrines, energies, eng, engproc, entomology, entropy, environments, environsciproc, epidemiologia, epigenomes, est, fermentation, fibers, fintech, fire, fishes, fluids, foods, forecasting, forensicsci, forests, foundations, fractalfract, fuels, future, futureinternet, futurepharmacol, futurephys, futuretransp, galaxies, games, gases, gastroent, gastrointestdisord, gels, genealogy, genes, geographies, geohazards, geomatics, geosciences, geotechnics, geriatrics, grasses, gucdd, hazardousmatters, healthcare, hearts, hemato, hematolrep, heritage, higheredu, highthroughput, histories, horticulturae, hospitals, humanities, humans, hydrobiology, hydrogen, hydrology, hygiene, idr, ijerph, ijfs, ijgi, ijms, ijns, ijpb, ijtm, ijtpp, ime, immuno, informatics, information, infrastructures, inorganics, insects, instruments, inventions, iot, j, jal, jcdd, jcm, jcp, jcs, jcto, jdb, jeta, jfb, jfmk, jimaging, jintelligence, jlpea, jmmp, jmp, jmse, jne, jnt, jof, joitmc, jor, journalmedia, jox, jpm, jrfm, jsan, jtaer, jvd, jzbg, kidneydial, kinasesphosphatases, knowledge, land, languages, laws, life, liquids, literature, livers, logics, logistics, lubricants, lymphatics, machines, macromol, magnetism, magnetochemistry, make, marinedrugs, materials, materproc, mathematics, mca, measurements, medicina, medicines, medsci, membranes, merits, metabolites, metals, meteorology, methane, metrology, micro, microarrays, microbiolres, micromachines, microorganisms, microplastics, minerals, mining, modelling, molbank, molecules, mps, msf, mti, muscles, nanoenergyadv, nanomanufacturing,\gdef\@continuouspages{yes}} nanomaterials, ncrna, ndt, network, neuroglia, neurolint, neurosci, nitrogen, notspecified, %%nri, nursrep, nutraceuticals, nutrients, obesities, oceans, ohbm, onco, %oncopathology, optics, oral, organics, organoids, osteology, oxygen, parasites, parasitologia, particles, pathogens, pathophysiology, pediatrrep, pharmaceuticals, pharmaceutics, pharmacoepidemiology,\gdef\@ISSN{2813-0618}\gdef\@continuous pharmacy, philosophies, photochem, photonics, phycology, physchem, physics, physiologia, plants, plasma, platforms, pollutants, polymers, polysaccharides, poultry, powders, preprints, proceedings, processes, prosthesis, proteomes, psf, psych, psychiatryint, psychoactives, publications, quantumrep, quaternary, qubs, radiation, reactions, receptors, recycling, regeneration, religions, remotesensing, reports, reprodmed, resources, rheumato, risks, robotics, ruminants, safety, sci, scipharm, sclerosis, seeds, sensors, separations, sexes, signals, sinusitis, skins, smartcities, sna, societies, socsci, software, soilsystems, solar, solids, spectroscj, sports, standards, stats, std, stresses, surfaces, surgeries, suschem, sustainability, symmetry, synbio, systems, targets, taxonomy, technologies, telecom, test, textiles, thalassrep, thermo, tomography, tourismhosp, toxics, toxins, transplantology, transportation, traumacare, traumas, tropicalmed, universe, urbansci, uro, vaccines, vehicles, venereology, vetsci, vibration, virtualworlds, viruses, vision, waste, water, wem, wevj, wind, women, world, youth, zoonoticdis 
% For posting an early version of this manuscript as a preprint, you may use "preprints" as the journal. Changing "submit" to "accept" before posting will remove line numbers.

%---------
% article
%---------
% The default type of manuscript is "article", but can be replaced by: 
% abstract, addendum, article, book, bookreview, briefreport, casereport, comment, commentary, communication, conferenceproceedings, correction, conferencereport, entry, expressionofconcern, extendedabstract, datadescriptor, editorial, essay, erratum, hypothesis, interestingimage, obituary, opinion, projectreport, reply, retraction, review, perspective, protocol, shortnote, studyprotocol, systematicreview, supfile, technicalnote, viewpoint, guidelines, registeredreport, tutorial
% supfile = supplementary materials

%----------
% submit
%----------
% The class option "submit" will be changed to "accept" by the Editorial Office when the paper is accepted. This will only make changes to the frontpage (e.g., the logo of the journal will get visible), the headings, and the copyright information. Also, line numbering will be removed. Journal info and pagination for accepted papers will also be assigned by the Editorial Office.

%------------------
% moreauthors
%------------------
% If there is only one author the class option oneauthor should be used. Otherwise use the class option moreauthors.

%---------
% pdftex
%---------
% The option pdftex is for use with pdfLaTeX. Remove "pdftex" for (1) compiling with LaTeX & dvi2pdf (if eps figures are used) or for (2) compiling with XeLaTeX.

%=================================================================
% MDPI internal commands - do not modify
\firstpage{1} 
\makeatletter 
\setcounter{page}{\@firstpage} 
\makeatother
\pubvolume{1}
\issuenum{1}
\articlenumber{0}
\pubyear{2023}
\copyrightyear{2023}
%\externaleditor{Academic Editor: Firstname Lastname}
\datereceived{ } 
\daterevised{ } % Comment out if no revised date
\dateaccepted{ } 
\datepublished{ } 
%\datecorrected{} % For corrected papers: "Corrected: XXX" date in the original paper.
%\dateretracted{} % For corrected papers: "Retracted: XXX" date in the original paper.
\hreflink{https://doi.org/} % If needed use \linebreak
%\doinum{}
%\pdfoutput=1 % Uncommented for upload to arXiv.org

%=================================================================
% Add packages and commands here. The following packages are loaded in our class file: fontenc, inputenc, calc, indentfirst, fancyhdr, graphicx, epstopdf, lastpage, ifthen, float, amsmath, amssymb, lineno, setspace, enumitem, mathpazo, booktabs, titlesec, etoolbox, tabto, xcolor, colortbl, soul, multirow, microtype, tikz, totcount, changepage, attrib, upgreek, array, tabularx, pbox, ragged2e, tocloft, marginnote, marginfix, enotez, amsthm, natbib, hyperref, cleveref, scrextend, url, geometry, newfloat, caption, draftwatermark, seqsplit
% cleveref: load \crefname definitions after \begin{document}

%=================================================================
% Please use the following mathematics environments: Theorem, Lemma, Corollary, Proposition, Characterization, Property, Problem, Example, ExamplesandDefinitions, Hypothesis, Remark, Definition, Notation, Assumption
%% For proofs, please use the proof environment (the amsthm package is loaded by the MDPI class).

%=================================================================
% Full title of the paper (Capitalized)
\Title{Representation learning for crop type classification of multispectral multitemporal remote sensing data}

% MDPI internal command: Title for citation in the left column
\TitleCitation{Title}

% Author Orchid ID: enter ID or remove command
\newcommand{\orcidauthorA}{0000-0001-9961-4763} % Add \orcidA{} behind the author's name
\newcommand{\orcidauthorB}{0000-0002-9813-7712} % Add \orcidB{} behind the author's name
\newcommand{\orcidauthorC}{0000-0003-2169-8009}
\newcommand{\orcidauthorD}{0000-0001-9703-3973}
% Authors, for the paper (add full first names)
\Author{Andrea González-Ramírez $^{1,*}$\orcidA{}, Clement Atzberger $^{2}$\orcidC{}, Deni Torres-Roman $^{1}$\orcidB{} and Josué López$^{2}$\orcidD{}}

%\longauthorlist{yes}

% MDPI internal command: Authors, for metadata in PDF
\AuthorNames{Firstname Lastname, Firstname Lastname and Firstname Lastname}

% MDPI internal command: Authors, for citation in the left column
\AuthorCitation{Lastname, F.; Lastname, F.; Lastname, F.}
% If this is a Chicago style journal: Lastname, Firstname, Firstname Lastname, and Firstname Lastname.

% Affiliations / Addresses (Add [1] after \address if there is only one affiliation.)
\address{%
$^{1}$ \quad Center for Research and Advanced Studies of the National 
Polytechnic Institute, Telecommunications Group, Av del Bosque 1145, 
Zapopan 45017, Mexico\\
% $^{2}$ \quad University of Natural Resources and Life Science, Institute of Geomatics, 
% Peter Jordan 82, Vienna 1180, Austria\\
$^{2}$ \quad Mantle Labs, Grünentorgasse 19, Vienna 1090, Austria}

% Contact information of the corresponding author
\corres{Correspondence: andrea.gonzalez@cinvestav.mx}%, deni.torres@cinvestav.mx and clement@mantle-labs.com}

% Current address and/or shared authorship
% \firstnote{Current address: Affiliation 3.} 
% \firstnote{These authors contributed equally to this work.}
% The commands \thirdnote{} till \eighthnote{} are available for further notes

%\simplesumm{} % Simple summary

%\conference{} % An extended version of a conference paper

% Abstract (Do not insert blank lines, i.e. \\) 
\abstract{Remote sensing (RS) spectral time series are a substantial source of information for earth monitoring tasks. % such as: land use and land cover classification, change detection, forest monitoring, among others. 
Supervised deep learning algorithms with large number of training samples are usually used to develop accurate solutions for these applications.
However, such approaches often face the lack of reliable labeled datasets. 
In addition, RS images acquired by optical sensors are frequently degraded by clouds/shadows, producing missing observations of an area of interest, creating irregular observation pattern.
To address these issues, efforts have been made to implement frameworks that generate meaningful representations from the available data and alleviate the deficiencies of the data sources and supervised algorithms.
Here, we propose a conceptually and computationally simple representation learning (RL) approach based on autoencoders (AEs) to generate informative and discriminative features for crop type classification using only a tiny set of reference samples.
Our AEs architecture has very few parameters compared to other models proposed in the state-of-the-art, leading to a scalable model able to process very large areas in low computational time.
%\textcolor{orange}{Different to other papers, we propose a representation learning model based on autoencoders (AEs) to create informative and discriminative features for crop type classification.} In addition, \textcolor{orange}{the architecture is configured to keep very low number of parameters compared to other models proposed in the state of the art}, producing a scalable model able to process very large areas in low computational time.
The proposed methodology ensembles a set of single layer AEs with very limited number of neurons, each one trained with mono temporal spectral features of a set of samples per class.
% belonging to a specific crop type, with the aim of reducing reconstruction difference for samples from the same crop type. 
The reconstruction difference vector between input samples and its corresponding estimation are averaged over all cloud/shadows free temporal observations of a pixel location. This averaged reconstruction difference vector is the base for the representations. 
Experimental results show that the proposed extremely light-weight architecture indeed generates separable features for competitive performances in crop type classification. A simple classification fully connected network (FCN) was trained and tested with representations generated for the Sentinel-2 multispectral multitemporal dataset named BreizCrops. 
Our method reaches $74.44\%$ overall accuracy which is $1\%$ higher than a convolutional neural network OmniscCNN, and $5.56\%$ lower than a transformer model, while our method is composed by $6.8k$ parameters, $\sim 400x$ less than the OmnicsCNN and $\sim 27x$ less than Transformers. These results prove that our method is competitive in classification performance compared with state-of-the-art methods while requiring much lower computational load.}
% Keywords
\keyword{crop types; multispectral time series; autoencoder; representation learning; reconstruction} 

% The fields PACS, MSC, and JEL may be left empty or commented out if not applicable
%\PACS{J0101}
%\MSC{}
%\JEL{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Diversity
%\LSID{\url{http://}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Applied Sciences
%\featuredapplication{Authors are encouraged to provide a concise description of the specific application or a potential application of the work. This section is not mandatory.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Data
%\dataset{DOI number or link to the deposited data set if the data set is published separately. If the data set shall be published as a supplement to this paper, this field will be filled by the journal editors. In this case, please submit the data set as a supplement.}
%\datasetlicense{License under which the data set is made available (CC0, CC-BY, CC-BY-SA, CC-BY-NC, etc.)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Toxins
%\keycontribution{The breakthroughs or highlights of the manuscript. Authors can write one or two sentences to describe the most important part of the paper.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Encyclopedia
%\encyclopediadef{For entry manuscripts only: please provide a brief overview of the entry title instead of an abstract.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Advances in Respiratory Medicine
%\addhighlights{yes}
%\renewcommand{\addhighlights}{%

%\noindent This is an obligatory section in “Advances in Respiratory Medicine”, whose goal is to increase the discoverability and readability of the article via search engines and other scholars. Highlights should not be a copy of the abstract, but a simple text allowing the reader to quickly and simplified find out what the article is about and what can be cited from it. Each of these parts should be devoted up to 2~bullet points.\vspace{3pt}\\
%\textbf{What are the main findings?}
% \begin{itemize}[labelsep=2.5mm,topsep=-3pt]
% \item First bullet.
% \item Second bullet.
% \end{itemize}\vspace{3pt}
%\textbf{What is the implication of the main finding?}
% \begin{itemize}[labelsep=2.5mm,topsep=-3pt]
% \item First bullet.
% \item Second bullet.
% \end{itemize}
%}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% The order of the section titles is different for some journals. Please refer to the "Instructions for Authors” on the journal homepage.

\section{Introduction}
Monitoring and analysis of the Earth's surface using remote sensors has been increasingly used for crop surface studies, 
given the quantity and availability of spectral-temporal images. 
The multi-spectral time series from sensors such as Landsat or Sentinel-2 have provided very cost-effective technical support to achieve the reliable identification and monitoring of large cropping areas \cite{Vuolo2018, Pelletier2019, Foerster2012, Chen2022}. 
While a wide number of data sources and 
supervised classification algorithms have been used for crop mapping \cite{Hu2016,Palchowdhuri2018,Heupel2018,Li2023,Dong2020,Paris2020,Russwurm2020,Foerster2012,Chen2022}, limited efforts have been made in feature selection as well as the use of un- and self-supervised learning algorithms to alleviate scarcity of labels and missing data produced by clouds.
Notable overviews and examples are provided in \cite{Yin2020,Yi2020,He2022, Dumeur2024, Wang2019}.

The use of multitemporal observations of multispectral data has a strong impact in crop type classification since the spectral difference in the crop growth over time are highlighted \cite{Vuolo2018,Yi2020}.
Each crop type has a distinct seasonal spectral behavior depending on local weather conditions \cite{Foerster2012}. 
Therefore, many researchers center their works on making use of multi-temporal information instead of
using single acquisition \cite{Vuolo2018, Dong2020, Hu2016, Roy2020}.

The most common methods for crop type classification are based on supervised learning algorithms \cite{Feng2019,Kussul2017,Cai2018,ManishLad2022,Inglada2017}. 
The aim of these algorithms is to train a discriminative model using labeled data. However, it is complicated to find tagged datasets, 
since it requires human accurate intervention, and normally datasets contain a huge amount of samples, which results 
in an expensive and time consuming task. Examples of supervised machine learning (ML) models include
decision trees (DT) \cite{Rokach2005}, random forest (RF) \cite{Breiman2001}, support vector machine (SVM) \cite{cortes1995} and artificial neural networks (ANN) \cite{Rosenblatt1958}.
The mentioned algorithms provide usually similar classification performance, but require extensive prepocessing steps such as compositing and gap-filling when incomplete (e.g., cloud-corrupted) time series are analyzed.

Unsupervised algorithms, such as autoencoders (AEs), mitigate the reliance on labeled datasets, although in principle these algorithms 
are not designed for the same purpose as supervised ones. An AE has as objective to compress data into a lower dimensional 
space, known as code, and then reconstruct the input \cite{ML2023}.  The code is regarded to be a set of features, also called representations, 
which condense the necessary information to recover the original data \cite{LopezPinaya2020}.

AEs have been widely used as change detection methods by generating representations from the reconstruction difference of 
samples that belong to a particular probability distribution \cite{LopezFandino2018,Luppino2024,Kalinicheva2019}. 
Moreover, representation learning (RL) is a broad subfield in machine learning, which is a set of techniques focused on 
automatically learning and identifying meaningful features from the input data. The field is closely related to the learning of low-dimensional manifolds within high-dimensional feature spaces
\cite{Swope2021, Bengio2013, Neumann2019, Li2022, Bengio2012, Engelen2019, Ericsson2022}.

In this work, we propose to train a light-weight deep learning model with individual time-tagged spectral signatures, while bypassing gap-filling and compositing methods.
% to estimate missing data caused by clouds.
In our framework, we use an ensemble of AEs to generate new informative and discriminative features for crop type classification. 
Here, we arbitrarily chose one simple AE per class, any other number of AEs would also be possible.
Instead of using the AE codes as representations, we calculate the respective reconstruction difference vector between the input and the output and use this concatenated reconstruction difference vector from the different AEs as representations. 
We evaluate the perfomance of the derived representations with a simple fully connected network (FCN) using the Breizhcrops dataset \cite{Russwurm2020}. 
We compare the outcomes against a number of competing approaches using the same dataset.


\subsection{Related work}
The authors of \cite{Russwurm2020}, propose a satellite image time series dataset for crop type mapping named BreizhCrops. 
They extract time series from Sentinel-2 at both pre-processing levels: top- (TOA) and bottom-of-atmosphere (BOA). 
They then use this dataset to benchmark a series of seven classification algorithms including RF and six deep learning 
methods based either on convolution, recurrence, or attention models for building a state-of-the-art benchmark on methods 
for crop type mapping.

The authors of \cite{Paris2020} address the challenges in crop type classification, such as the presence of clouds that
corrupts the multi-temporal spectral signature of remote sensing images and the necessity of labeled 
samples, using Sentinel-2 time series and a Long-short term memory (LSTM) model.

% With respect to feature selection, next to spectral and temporal features, textural and environmental features are often used to characterize the crop growth characteristics \cite{Yi2020}.
In \cite{He2022} a method to merge spectral, textural and environmental features is proposed, with the purpose of designing a more accurate and efficient crop 
type classification method.

Swope et al. \cite{Swope2021} propose a new self-supervised training, named contrastive sensor fusion, wich is a technique 
for learning unsupervised representations through a "Siamese network" training scheme. 
They used shared information from multiple sensors and spectral bands by training a single model to produce a representation that remains similar 
when any subset of its input channels is used. 
\cite{Yuan2022} propose a method called SITS-Former, that is pre-trained with unlabeled Sentinel-2 time series data to learn spatio-spectral-temporal features via a missing-data imputation proxy task based on self-supervised learning.
\cite{Lisaius2024} use representations derived from a spectral-temporal Barlow Twin (STBT) for crop type classification and RL.

The research in \cite{Kalinicheva2019} is particularly interesting in the use of AEs. They propose an approach that uses the reconstruction
losses of joint AEs to detect non-trivial changes (permanent changes and seasonal changes that do not follow common tendency) between two 
co-registered images in a satellite image time series.

An approach using AEs for unsupervised feature-learning in hyperspectral data was proposed in \cite{Windrim2019}.
The method permits to evaluate the separability of the feature spaces for clustering tasks.

% Images from optical sensors are often contaminated with clouds and shadows \cite{Paris2020}. Therefore, it is necessary 
To address the problem of missing data, other techniques in the state, such as the ones based on combination of optical and SAR data \cite{Begue2018, Orynbaikyzy2019, Kussul2017}, 
fusion of multiple sensors \cite{PierrePott2022, Heupel2018,MorenoMartinez2020} or data interpolation \cite{Russwurm2020, Kandasamy2013} have been developed.
It is worth mentioning that sensor combination/fusion implies the enormous challenge of handling huge amounts of data,
which leads to higher computational load and increases in processing time.

\begin{table}[H]
	\caption{Summary of relevant works related to crop types classification and representation learning.}
	\tiny
	\resizebox{\textwidth}{!}{%
	\centering
	\begin{tabular}{p{1.7cm}|c|c|c|c|c}
	\hline
		\textbf{References}(year)[cites] & \textbf{Satellite} & \makecell{\textbf{Time} \\ \textbf{range}} & \textbf{Method} & \makecell{\textbf{Number} \\ \textbf{of classes}} & \makecell{\textbf{Feature} \\ \textbf{selection}} \\ \hline
		Kalinicheva, E., et al, (2019)[19] \cite{Kalinicheva2019}. &  SPOT-5 & \makecell{2002\\2008} & AEs & Not specified & N/A \\ \hline
		Windrim, E., et al, (2019)[42] \cite{Windrim2019}. &  \makecell{AVIRIS\\and\\others} & Not specified & AEs & Not specified & N/A \\ \hline		
		Paris, Claudia, et al (2020)[13] \cite{Paris2020}. & S2 & \makecell{09/2017 \\ 08/2018} & LSTM & 12 & N/A \\ \hline
		Russwurm, Marc, et al. (2020)[65]. \cite{Russwurm2020}& S2 & \makecell{01/01/2017 \\ 31/12/2017}& \makecell{RF\\and\\others} & 9 & N/A \\ \hline
		Zhiwei Yi, et al. (2020)[56]. \cite{Yi2020}& S2 & \makecell{23/04/2019 \\ 20/09/2019}& RF & 8 & \makecell{Spectro\\temporal} \\ \hline
		Shan He, et al. (2022)[3]. \cite{He2022} & MODIS & \makecell{01/01/2009 \\ 31/12/2009} & KS & 4 & \makecell{Spectral \\ textural \\ environmental} \\ \hline
		Leikun Yin, et al. (2020)[35]. \cite{Yin2020}& S2 & \makecell{01/04/2018 \\ 31/10/2018} & RF & 3 & \makecell{Spectro \\ temporal} \\ \hline
		Lisaius, et al. (2024)[-]. \cite{Lisaius2024}& S2 & \makecell{01/01/2017 \\ 31/12/2018} & STBT & 8 & \makecell{Spectro \\ temporal} \\ \hline
		\textbf{Proposal in this work} & S2 & \makecell{01/01/2017 \\ 31/12/2017} & AEs & 9 & N/A \\ \hline
	\end{tabular}}
\end{table}

\subsection{Contributions}
The main contributions of this work are the following:
\begin{enumerate}
	\item To tackle cloud-corrupted time series analysis, the proposed framework processes individual time-tagged spectral signatures for feature extraction and thereby completely avoids the use of gap-filling and compositing methods.
	\item The proposed methodology uses neural networks with a reduced number of neurons to keep the computational load low, thereby facilitating the processing of large geographic areas.
	\item The proposed pixel-wise framework provides a robust solution with respect to the number of available cloud-free observations, while achieving competitive results even under high levels of cloudiness.
\end{enumerate}

The remainder of this work is organized as follows. Section 2 presents the concept of RL and the respective mathematical definitions, as well as a brief description of AEs. 
In Section 3 the problem statement of this work and the mathematical formulation of the proposed framework are introduced. 
Section 4 describes quantitative and qualitative experimental results. 
Sections 5 and 6 present the discussion and conclusions, respectively, of the results obtained in our experiments.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Materials and Methods}
\subsection{Representation Learning}
Representation learning, also called feature learning, is a subfield of machine learning
%RL is the process of deriving a representation from input data tailored to specific tasks, it involves extracting significant information to connect low-level data with higher-level semantic concepts \cite{Tzelepi2022}.
%\textcolor{orange}{"It refers to the process of learning a representation from an input object toward a specific task" \cite{Tzelepi2022}.} 
that aims to automatically learn and identify meaningful features, or representations, from the input data.
Representations are expected to be more informative for downstream tasks such as clustering, regression, or classification \cite{Tzelepi2022}. 

\begin{figure}[H]
	\centering
	\includegraphics[width=9cm]{figures/representation_learning.pdf}
	 \caption{Illustration of RL as a function $f$, mapping vectors from a dimensional space to a representation space.}
	\label{}      
	\centering
\end{figure}

Mathematically, RL is defined as a function $f : \mathbf{X} \to \mathbf{Z}$, that transforms the input data $\mathbf{X} = \{\mathbf{x}_1, \dots, \mathbf{x}_S\}$, into features $\mathbf{Z} = \{\mathbf{z}_1, \dots, \mathbf{z}_S\}$, where each vector $\mathbf{x}_s \in \mathbb{R}^n$ and its image $\mathbf{z}_s \in \mathbb{R}^p$, and $\mathbb{R}^p$ denotes the representation space.
The objective function $f$ leads the model to learn meaningful representations of the input data, preserving information, reducing redundancy and generally reducing dimensionality.

In recent years, many RL methods have been proposed from different perspectives / families \cite{Balestriero2023}, 
e.g., contrastive learning methods (InfoNCE \cite{Tschannen2019,LeKhac2020,Aitchison2021}), deep metric learning (SimCLR \cite{Chen2020,Bachman2019}, NNCLR \cite{Dwibedi2021}, etc.), non-contrastive methods (VICReg \cite{Bardes2021}, BarlowTwins \cite{Zbontar2021,Lisaius2024}, etc), among others.
Such approaches are particularly useful in cases where the observed data is generated by a limited set of variables \cite{Coifman2006}. 
However, RL is not limited to these families of methods, and conventional neural network models, such as autoenconders can form a representation 
learning method.

\subsection{Autoencoders}\label{aes}
AEs are a specific type of ANN used for unsupervised learning (Figure \ref{AEs_example}) \cite{Bank2020}. They have applications in various research fields, such as anomaly detection, 
data compression, and feature learning. Their aim is to encode the input into a compressed representation, and then reconstruct the input from this representation, so that the reconstruction is as similar as possible to the input \cite{Tzelepi2022,Zhang2019}. 
Both under- and overcomplete versions exist, as well as variational AE \cite{Valero2021}.

% The AEs are composed by the following elements:
% \begin{itemize}
% 	\item \textbf{Encoder}: The input data is passed through an encoder network, which reduces the dimensionality of the input and produces a compressed representation.
% 	\item \textbf{Code}: The output of the encoder is a compressed representation of the input data, also known as the "latent space" or "bottleneck." This encoding captures the essential features of the input in a low dimension space.
% 	\item \textbf{Decoder}: The code is passed through the decoder,  which aims to reconstruct the original input data. The decoder's architecture is typically a mirror image of the encoder, gradually expanding the dimensions back to match the original input.
% 	\item \textbf{Loss function}: The performance of an AE is typically measured by a loss function, which quantifies the difference between the input and the reconstructed output.
% 	\item \textbf{Training}: During training, the AE adjusts its weights to minimize the reconstruction error, effectively learning a compact representation of the input data.\\
% \end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=8cm]{figures/autoencoder_methodology.pdf}
	 \caption{Example of an AE architecture with mathematical definition as a function. In the present work, not the codes are used as representations, but the recostruction difference between input and output.}
	\label{AEs_example}      
	\centering
\end{figure}

The aim of the AEs, formally defined in \cite{Baldi2012}, is to learn  the functions $f_{E}: \mathbb{R}^{n} \to \mathbb{R}^{p} $ and $f_{D}: \mathbb{R}^{p} \to \mathbb{R}^{n}$, where $f_{E}$ denotes the encoder function, $f_{D}$ is the decoder function, $n$ is the input and output dimension, and $p$ denotes the dimension of the code.
Generally $p \ll n$, leading to learn compressed features of the data. 
% A characteristic of the AEs is that they are trained gradually layer by layer, therebefore, each hidden layer is seen as a regression model.

% \subsubsection{Encoder}
% The input data is passed through an encoder network, which reduces the dimensionality of the input and produces a compressed representation.
% This function $f_E$ maps the input data to the learned representation, each neuron is conected with all neurons of the next layer, 
% the conection between layers have a weight and bias  associeted and each neuron apply to the input one activation function alpha.
% Mathematically, it can be represented as:

% \begin{equation}
% 	f_{E_{i}}(x) = \alpha(w_{i}x + b_{i})
% \end{equation}
% where $i = 1,2, \dots , I$ is the number of layers, $w_{i}$ denotes the weight, $b_{i}$ is the bias and $\alpha$ denotes the activation function.
% Therefore, we can define $f_{E{i}} : \mathbb{R}^{n_{i-1}} \to \mathbb{R}^{n_{i}}$ considering $n_{i} < n_{i-1}$.

% % \begin{itemize}
% % 	\item Each neuron is conected with all neurons of the next layer.
% % 	\item The conection between layers have a weight $w_{i}$ and bias $b_{i}$ associeted.
% % 	\item Each neuron apply to the input one activation function $\alpha$
% % 	\item The adjust of the weights is with the backpropagation learing algorithm. 
% % \end{itemize} 

% \subsubsection{Bottleneck}
% The output of the encoder is a compressed representation of the input data, 
% also known as the "latent space" or "code." This encoding captures the essential 
% features of the input in a low dimension space. 

% % \begin{equation}
% % 	n_{1} = m_{J}                  
% % 	n_{2} = m_{j-1}                                 
% % 	\vdots                                       
% % 	n_{I} = m_{j} 
% % \end{equation}

% \subsubsection{Decoder}
% The code is passed through the decoder function $f_{D}$, which aims to reconstruct the original input data. 
% The decoder's architecture is typically a mirror image of the encoder, gradually expanding the 
% dimensions back to match the original input, therefore, they have a symmetric architecture. 
% This function is Mathematically defined as following:

% \begin{equation}
% 	f_{D_{j}}(x) = \alpha(w_{j}x + b_{j})
% \end{equation}
% where, $j = 1,2, \dots , J$ is the number of layers, $w_{j}$ denotes the weight, $b_{j}$ is the bias and $\alpha$ denotes the activation function.
% We define $f_{D{j}} : \mathbb{R}^{m_{j-1}} \to \mathbb{R}^{m_{j}}$ considering $m_{j} < m_{j-1}$.


% \subsubsection{Loss Function}
% The performance of an AE is typically measured by a loss function, which 
% quantifies the difference between the input and the reconstructed output.
% The loss function is typically defined using a distance metric, it can be mathematically defined as:

% % \begin{equation}
% % 	RE = L(X, \tilde{X}) %f_{D}(f_{E}(X)))
% % \end{equation}

% %where $L$ is a loss function, $X$ is the input data and $\tilde{X}$ is the reconstructed data.

% %Re = L(X,g(f(X)))

% \subsubsection{Training}
% During training, the AE adjusts its weights to minimize the reconstruction 
% error, effectively learning a compact representation of the input data.


% \begin{table}[H]
% 	\centering
% 	\small
% 	\begin{tabular}{c c c}
% 	   % &\begin{figure}\includegraphics[width=3cm]{Figures/concepts_images/AE_methodology_part2.pdf}\centering\end{figure}& \\
% 		\textbf{Encoder} & & \textbf{Decoder} \\ 
% 		Linear regression model & & Linear regression model\\ [1ex]
% 		$f_{i}(x) = \alpha(w_{i}x + b_{i})$ &  & $f_{j}(x) = \alpha(w_{j}x + b_{j})$ \\
% 		$i = 1,2, \dots , I$  & & $j = 1,2, \dots , J$ \\ 
% 		where $I$ is the number of layers & & where $J$ is the number of layers\\
% 		$f_{i} : \mathbb{R}^{n_{i-1}} \to \mathbb{R}^{n_{i}}$ & & $f_{j} : \mathbb{R}^{n_{j-1}} \to \mathbb{R}^{n_{j}}$\\ 
% 		$n_{i} < n_{i-1}$ &  $\Longleftarrow$  \textbf{Bottleneck} $\Longrightarrow$  & $m_{j} < m_{j_1}$\\
% 						  &   $n_{1} = m_{J}$                                & \\ 
% 						  & $n_{2} = m_{j-1}$                                & Symmetric architecture\\ 
% 						  & $\vdots$                                         & $I = J$\\  
% 						  & $n_{I} = m_{j}$                                  & \\ 
% 	\end{tabular}
% 	\caption{}
% \end{table}

\section{Problem statement and proposed method}
Consider a multi-spectral multi-temporal dataset acquired by an optical sensor, where each sample has been acquired at different times. From the entire set of observations, only a subset will usualy be useful as climate conditions such as clouds, cirrus, cloud shadows, snow, among others, occasionally obstruct the land surface. 
Missing data produced by these conditions commonly leads to poor perfomance on particular tasks, such as land use / land cover classification or change detection.
Therefore, it is of utmost importance to extract and  use only the land related information, either by filtering the data, or generating new features (often in the form of composites).

\subsection{Mathematical formulation}
Let $\mathscr{X} \in \mathbb{R}^{P \times B \times T}$ be a multispectral time series dataset represented as a third-order array, where $P$ represents the number of geographic points on the earth surface, $B$ is the number of spectral bands, and $T$ denotes the number of temporal observations, and each geographic point is denoted as a vector $\mathbf{x} \in \mathbb{R}^{B \cdot T}$ and $\mathbf{x} \in \mathscr{X}$. The aim is to transform each vector $\mathbf{x}$ into a representation vector $\mathbf{z} \in \mathbb{R}^{R}$, where $R$ is the number of new features named representations, to address label scarcity and missing data produced by clouds, and to alleviate tasks such as crop type classification (See Figure \ref{bigpicture}).

\begin{figure}[h!]
	\centering
	\includegraphics[width=10cm]{figures/bigpicture_with_clement.pdf}
	\caption{First level proposed workflow. Scene classification product provided by the European Space Agency (ESA) is used to mask out cloudy samples from a geographic point (pixel) shaped as a $T \times B$ array. The pixel cloud free observations are the inputs to generate a representations vector, which is a concatenation of the reconstruction differences per AE, and are then the input data to a classification model.}
	\label{bigpicture}
	\centering
\end{figure}

\subsection{Methodology}
The methodolgy of this work consists of four processes: data downloading/prepocessing, model training, inference (representations formation) and, as downstream task, classification. The proposed framework is shown in more detail in Figure \ref{abstract}.
\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{figures/abstract.pdf}
	\caption{Proposed framework block diagram. The full methodology is composed by four main blocks: data preprocessing, model training, representation generation and evaluation.} 
	\label{abstract}
\end{figure}
\subsubsection{Data downloading/preprocessing}\label{data_preprocessing}
Reference, crop type labels were extracted from a public benchmark dataset named BreizCrops \cite{Russwurm2020} (field level).
Google earth engine (GEE) was used to download full multitemporal multispectral data from a region of interest (ROI) (see Figure \ref{GEE_process}). \\
\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\textwidth]{figures/gee_download.pdf}
	\caption{Process of downloading datasets using the GEE API.}
	\label{GEE_process}    
\end{figure}

Given the multispectral multitemporal remote sensing dataset, a scene classification product (e.g., sen2cor) is used to get a cloud/non-cloud mask for each sample $\mathbf{x}$, i.e., for each temporal observation at pixel level.
Cloudy samples are excluded from the dataset producing a set of pixels with variable number of cloud free temporal observations.

To leverage the temporal information for the particular task of crop type classification, we add temporal embeddings to each sample, which aim to extend the vector space to one where similar spectral curves of different crop types at different growth stages are separable.
Hence, each sample $\mathbf{\tilde{x}} \in \mathbb{R}^F$
has $F$ features, i.e., $B$ spectral bands plus two values denoting the sensing Day Of Year (DOY), and are computed, given the cyclical character of the nature seasons, using sin and cosine functions as follows
\begin{equation}
	d_{\sin} = \left(\sin\left(\frac{2\pi d}{365}\right)+1\right)/2
\end{equation}
and
\begin{equation}
	d_{\cos} = \left(\cos\left(\frac{2\pi d}{365}\right)+1\right)/2, 
\end{equation}
where $d$ denotes the DOY as a numeric value from 1 to 365, and $d_{\sin}$ and $d_{\cos}$ are in the range 0 to 1.
	% \item Inference: In this part of the proposed framework,the representations are generated using the AEs trained. Given as a input individual temporal observation free of clouds to the AEs, after that, generating the recostruction and computing the error beteween input and recostruction, deleting the day of the year (DOY) and computing the average per pixel to generate the representations $\mathbf{Z}$.
	% \item Evaluation: There are two option, the first is to use distance metrics for evaluate the separability between classes and the second is to do a classification using a FCN and after that evaluate the prediction usign metrics for performance evaluation.

\subsubsection{Model training}
The principle of this work is to train a set of $C$ independent autoenconders with vectors $\mathbf{\tilde{x}}_c$ that belong to a particular class/cluster $c$ for $c=1,\dots,C$,  resulting in $C$ semi-supervised trained models able to reconstruct samples from the same class/cluster, approaching the reconstruction difference to zero, while using the ensemble of reconstruction differences to derive the representations (see Figure \ref{errors}).

\begin{figure}[H]
	\includegraphics[width=\textwidth]{figures/AE_example_corrected.pdf}
	\caption{Example of the expected output for positive and negative samples. The "errors" from the ensemble of AEs constitute the representations for the downstream task.}
	\label{errors}
\end{figure}
The training process can be semi-supervised, given a labeled dataset, as in this work, or unsupervised, with no ground truth data (e.g., by training a set of random AEs not associated to specific crop types or classes). 
The scope of this work addresses the semi-supervised approach, with a crop type labeled dataset as pairs $(x,y)$, where $y$ is an integer value which indicates the class that $x$ belongs to. Samples are split in as many subdatasets as classes and each subdataset is used to train a different AE.
This approach is graphically represented in Figure \ref{aes_train}. 
\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{figures/training.pdf}
	\caption{AEs training. Each AE is trained with a set of individual spectral curves belonging to one of the crop types. The reconstructions from the $C$ classes are used to calculate the difference vector across the ensemble, that is the final set of representations.}    
	\label{aes_train}      
\end{figure}

As mentioned in Section \ref{aes}, an AE is formally defined as a composed function $f(x) = f_D(f_E(x))$. For our input vectors $\mathbf{\tilde{x}}_c$, each AE can be seen as a function
\begin{equation}
	f(\mathbf{\tilde{x}}_c) = f_D(f_E(\mathbf{\tilde{x}}_c)))
\end{equation}
where, $f_E(\mathbf{\tilde{x}}_c)$ denotes the encoder function, which maps the input vectors that belong to class $c$, from the input space $\mathbb{R}^F$ to an embedding space $\mathbb{R}^P$, known as code,
and, at the same time, it is a composed function
\begin{equation}
	f_E(\mathbf{\tilde{x}}_c) = f_{E_{I}}(f_{E_{I-1}}(\dots (f_{E_{1}}(\mathbf{\tilde{x}}_c))))
	\label{composed}
\end{equation}
where $f_{E_{i}}(\cdot)$, for $i = 1, \dots, I$ are regression functions
\begin{equation}
	f_{E_{i}}(\mathbf{y}_{i-1}) = \alpha_i(\mathbf{W}_{i}\mathbf{y}_{i-1} + \mathbf{b}_{i})
\end{equation}
where $i$ denotes the layer number, $\mathbf{y}_{i-1}$ stands for the output of layer $i-1$, $\mathbf{W}_{i}$ and $\mathbf{b}_{i}$ denotes the weights and bias at layer $i$ respectively, and $\alpha_i$ the activation function.

The decoder function $f_D(\cdot)$, also a composed function as $f_E(\cdot)$ in eq. \ref{composed}, maps the code to a estimated reconstruction
\begin{equation}
	\mathbf{\hat{x}} = f_{D}(f_E(\mathbf{\mathbf{\tilde{x}}}_c))
\end{equation}
which is approximated to the input, updating the weights $\mathbf{W}_i$ and bias $\mathbf{b}_i$ by backpropagating the error $\mathbf{d}$ computed by a loss function $L$ as
\begin{equation}
	\mathbf{d} = L(\mathbf{\tilde{x}}_c, f_D(f_E(\mathbf{\tilde{x}}_c))) = L(\mathbf{\tilde{x}}_c, \mathbf{\hat{x}})
	\label{loss}
\end{equation}
where $L$ computes the distance between $\mathbf{\tilde{x}}_c$ and $\mathbf{\hat{x}}$, and $\mathbf{W}_i$ and $\mathbf{b}_i$ are updated approaching $\mathbf{d} \to \mathbf{0}$, by an optimization algorithm such as stochastic gradient descent, Adam, Adagrad.

\subsubsection{Inference (representations formation)}
Given the $C$ trained AEs, denoted as $f_c$, the set of cloud free observations of individual geographic points (pixels) form an array $\mathbf{\tilde{X}} \in \mathbb{R}^{t \times F}$, where $t$ denotes the number of cloud free samples for a given pixel. $\mathbf{\tilde{X}}$ is the input to all the AEs, and the reconstruction array is obtained from each AE as
\begin{equation}
	\mathbf{\hat{X}}_c = f_c(\mathbf{\tilde{X}}) 
\end{equation}
where $\mathbf{\hat{X}}_c$ represents the reconstruction estimated by AE $c$. Then, the difference vector is computed by the same loss function used in training phase (eq. \ref{loss}) as
\begin{equation}
    \mathbf{D}_c = L(\mathbf{\tilde{X}}, \mathbf{\hat{X}}_c)
\end{equation}
and the pixel mean reconstruction difference $\mathbf{\bar{d}}_c$ is computed by
\begin{equation}
	\mathbf{\bar{d}}_c = \frac{1}{t}\sum_{s=0}^{t}\mathbf{d}_{sc}
\end{equation} 
where $\mathbf{d}_{sc}$ denotes the $s$-th row of $\mathbf{D}_c$ and the representations are formed by concatenating the mean pixel reconstruction erros as
\begin{equation}
    \mathbf{z}=\mathbf{\bar{d}}_1 \oplus \mathbf{\bar{d}}_2 \oplus \cdots \oplus \mathbf{\bar{d}}_C
\end{equation}
where $\oplus$ denotes the vector concatenation and $\mathbf{\bar{d}_c}$ the mean pixel reconstruction difference vector from AE $c$.
The inference phase of our proposed framework is presented in Figure \ref{Inference}
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{figures/testing.pdf}
	\caption{Inference workflow of the proposed framework. For each temporal set of cloud-free reflectance spectra, the average reconstruction errors are calculated for each of the $C$ AEs and concatenated to define the representations of this pixel.}
	\label{Inference}    
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}
\subsection{Dataset}\label{dataset}
For this work the Breizhcrops dataset \cite{Russwurm2020} was used for experiments and evaluation. The provided multi-temporal multi-spectra data is from the Brittany region in the northwest of France and is composed of labeled Sentinel-2 images from January 1st to December 31st, 2017. Labels are assigned to the "average of reflectance values over the bounds of the field geometry retrieved from the dataset" \cite{Russwurm2020}.

This dataset is organized in four regions (see Table \ref{Regions}),
% by the "Nomenclature des Unités Territoriales Statistiques" (NUTS) system (see Table \ref{Regions}): Côtes-d’Armor (FRH01), Finistère (FRH02), Ille-et-Vilaine (FRH03),and Morbihan (FRH04)
and each region contains nine crop categories: barley, wheat, rapeseed, corn, sunflower, orchards, nuts, permanent meadows and temporary meadows.
To allow for a direct comparison to the work published in \cite{Russwurm2020}, we use the regions FRH01 and FRH02 for training, FRH03 for validation, and FRH04 for evaluation.
The data split is described in Table \ref{data_split}, which outlines the features employed in this experiment. These include DOY (sin and cosine), 10 spectral bands (10 and 20 meters resampled to 10m) and five well-known spectral indices (NDWI, NDVI, NDTI, NDSVI and EVI).

Table \ref{samples} describes the number of samples per class used for training, validation and test respectively.
It is worth noting that the dataset is imbalanced, i.e., each class has different number of samples. This makes the classification model more sensitive to overfitting and also makes an accuracy evaluation more difficult \cite{Foody2002}.
% \begin{table}[H]
% 	\caption{Regions of Britany with number of field parcels and time series for the atmospherically corrected surface reflectances at the bottom-of-atmosphere (L2A) \cite{Russwurm2020}.}
% 	\centering
% 	\begin{tabular}{c  c  c  c} 
% 		\hline
% 		Regions     & NUTS-3 & Parcels   & L2A \\ [1ex] 
% 		\hline
% 		Côtes-d'Armor   & FRH01  & $221,095$ & $178,632$ \\ [1ex]
% 		Finistère       & FRH02  & $180,565$ & $140,782$ \\ [1ex]
% 		Ille-et-Vilaine & FRH03  & $207,993$ & $166,367$ \\ [1ex]
% 		Morbihan		& FRH04  & $158,522$ & $122,708$ \\ [1ex] 
% 		\hline
% 		Total			&		 & $768,175$ & $608,489$ \\ [1ex] 
% 		\hline
% 	\end{tabular}
% 	\label{Regions}
% \end{table}
\begin{table}[H]
	\caption{Regions of Britany (France) with number of field parcels and spectral data for the atmospherically corrected surface reflectances at the bottom-of-atmosphere (L2A) \cite{Russwurm2020}. The regions FRH01 and FRH02 were used for training, FRH03 for validation, and FRH04 for evaluation.}
	\begin{minipage}[b]{.4\linewidth}
		\small
		\begin{tabular}{c  c  c} 
			\hline
			Regions     & NUTS-3 & L2A \\ [1ex] 
			\hline
			Côtes-d'Armor   & FRH01  & $178,632$ \\ [1ex]
			Finistère       & FRH02  & $140,782$ \\ [1ex]
			Ille-et-Vilaine & FRH03  & $166,367$ \\ [1ex]
			Morbihan		& FRH04  & $122,708$ \\ [1ex] 
			\hline
			Total			&		 & $608,489$ \\ [1ex] 
			\hline
		\end{tabular}
		\label{Regions}
	\end{minipage}
	\begin{minipage}[b]{.5\linewidth}
		\centering
		\includegraphics[width=0.9\linewidth]{figures/breizhCrops_regions_black.pdf}
	  \end{minipage}
\end{table}
\begin{table}[H]
	\centering
	\caption{Number of samples per class used for training, validation and test.}
	\small
	\begin{tabular}{c c c c c}
		\hline
		Class			& Training		& Validation 	& Test 		& Total \\
		\hline
		Barley			& 23,787		& 7,154			& 5,981		& 36,922 \\
		Wheat			& 45,406		& 27,202		& 17,009	& 89,617 \\
		Rapeesed		& 7,945			& 3,557			& 3,244		& 14,746 \\
		Corn			& 80,623		& 42,011		& 31,361	& 153,995 \\
		Sunflower		& 7				& 10			& 2			& 19 \\
		Orchards		& 1,285			& 1,217			& 552		& 3,054 \\
		Nuts			& 28			& 10			& 11		& 49 \\
		Perm. Meadows	& 69,177		& 32,524		& 25,134	& 126,835 \\
		Temp. Meadows	& 91,156		& 52,682		& 38,414	& 182,252 \\
		\hline
	\end{tabular}
	\label{samples}
\end{table}
It is worth mentioning that this datasets provides only spectral signatures in tabular format for the center pixel in a field and not Sentinel 2 images. Nevertheless, to enable our qualitative analysis and produce output classification maps, we downloaded Sentinel 2 images using GEE. However, not all Sentinel 2 products in 2017 are available in GEE database and less images than in Breizhcrops dataset were used for this experiment.
\subsection{AEs training.}
With the aim of developing an algorithm capable of being scalable to relatively large geographic areas, the AE is composed by a single layer FCN as encoder, and its counterpart for the decoder.
This keeps computational load and processing times lower than other models such as convolutional or recurrent networks.
Batch size, learning rate, number of units in hidden layer and the loss function were set in accordance with the results acquired through the hyperparameter random search presented in Appendix \ref{app_a}.
Table \ref{hyperparameters_aes} presents the AEs configuration.

\begin{minipage}[t]{0.5\textwidth}
	\begin{table}[H]
		\caption{Dataset split.}
		\begin{tabular}{c|c} 
			\hline
			Parameter & Value \\ [1ex] 
			\hline
			Training size & 319,414 \\ [1ex]
			Validation size & 166,367 \\[1ex]
			Testing size & 122,708 \\ [1ex]
			Features & \makecell{10 bands, \\ 2 DOY,\\ 5 spectral indices} \\ [1ex] 
			Classes & 9 \\ [1ex] 
			\hline
		\end{tabular}
	\label{data_split}
	\end{table}
\end{minipage}
\begin{minipage}[t]{0.35\textwidth}
	\begin{table}[H]
		\caption{Setting up of the AEs hyperparameters.}
		\begin{tabular}{c|c}
		\hline
			\multicolumn{2}{c}{Hyperparameters} \\ \hline
			Epochs & 1000 \\
			Early stop & True \\ 
			Patience & 10 \\ 
			Min. delta & 1e-5 \\ 
			Batch size rate & 0.05* \\ 
			Units in hidden layers & 5 \\ 
			Learning rate & 1e-4 \\ 
			Optimizer & Adam \\ 
			Loss & MAE \\ \hline
			\multicolumn{2}{c}{* proportion of samples for each class}
		\end{tabular}
		\label{hyperparameters_aes}
	\end{table}
\end{minipage}

% \subsection{Loss functions}
% Considering the regression loss functions Mean Absolute Error (MAE) and Mean Square Error (MSE) as the search elements in our random search, MAE is defined by
% as the loss function for this experiments, since it performed slightly better than mse in most of the runs.
Figure \ref{loss_fuctions} shows, for each AE, the training and validation loss function computed as the mean absolute error (MAE)
\begin{equation}
	MAE = \frac{1}{M}\sum_{m=0}^{M}|x_m - \hat{x}_m|
\end{equation}
% We can see a soft MAE decrease, both for the training (blue) and validation (orange) data. In some AEs (\ref{fig:loss_nuts}, \ref{fig:loss_sunflower}, \ref{fig:loss_orchards}), the loss function still decays at the last epoch, but was stopped according to our hyperparameters setting \ref{hyperparameters_aes}.
\begin{figure}[H]
	\begin{subfigure}[t]{0.3\textwidth}
		\includegraphics[width=\linewidth]{figures/results_040923/loss_barley.pdf}
	\caption{}
	\label{fig:loss_barley}
	\end{subfigure}\hfill
	\begin{subfigure}[t]{0.3\textwidth}
	  \includegraphics[width=\linewidth]{figures/results_040923/loss_corn.pdf}
	\caption{}
	\label{fig:loss_corn}
	\end{subfigure}\hfill
	\begin{subfigure}[t]{0.3\textwidth}
		\includegraphics[width=\linewidth]{figures/results_040923/loss_nuts.pdf}
	\caption{}
	\label{fig:loss_nuts}
	\end{subfigure}\\[0.2cm]
	\begin{subfigure}[t]{0.3\textwidth}
		\includegraphics[width=\linewidth]{figures/results_040923/loss_rapeseed.pdf}
	\caption{}
	\label{fig:loss_rapeseed}
	\end{subfigure}\hfill
	\begin{subfigure}[t]{0.3\textwidth}
		\includegraphics[width=\linewidth]{figures/results_040923/loss_permanent_meadows.pdf}
	\caption{}
	\label{fig:loss_pm}
	\end{subfigure}\hfill
	\begin{subfigure}[t]{0.3\textwidth}
		\includegraphics[width=\textwidth]{figures/results_040923/loss_temporary_meadows.pdf}
	\caption{}
	\label{fig:loss_tm}
	\end{subfigure}\\[0.2cm]
	\begin{subfigure}[t]{0.3\textwidth}
		\includegraphics[width=\linewidth]{figures/results_040923/loss_sunflower.pdf}
	\caption{}
	\label{fig:loss_sunflower}
	\end{subfigure}\hfill
	\begin{subfigure}[t]{0.3\textwidth}
		\includegraphics[width=\linewidth]{figures/results_040923/loss_orchards.pdf}
	\caption{}
	\label{fig:loss_orchards}
	\end{subfigure}\hfill
	\begin{subfigure}[t]{0.3\textwidth}
		\includegraphics[width=\linewidth]{figures/results_040923/loss_wheat.pdf}
	\caption{}
	\label{fig:loss_wheat}
	\end{subfigure}
	\caption{Loss functions of AEs trained with (a) barley, (b) corn, (c) nuts, (d) rapeseed, (e) permanent meadows, (f) temporary meadows, (g) sunflower (h) orchards and (i) wheat samples.}
	\label{loss_fuctions}
\end{figure}

\subsection{Separabilty assessment and distance metrics}
For qualitative assessment of the inter-class separability in the generated representations space, 
3D scatterplots of the test spectral-temporal RS data and their corresponding representations produced by our method, reduced to a three-dimensional space by principal component analysis (PCA), are shown in Figures \ref{fig:pca_raw} and \ref{fig:pca_rep} respectively.
The scatter plot of representations, compared with that of the initial data, shows that the density of points belonging to each of the crop types is much better clustered.
\begin{figure}[H]
	\centering
	\begin{subfigure}[t]{0.4\linewidth}
		\centering
		\includegraphics[width=0.9\linewidth]{figures/pcs_2.png}
		\caption{}
	  \label{fig:pca_raw}
	\end{subfigure}
	\begin{subfigure}[t]{0.5\linewidth}
		\centering
		\includegraphics[width=0.9\linewidth]{figures/PCA_3D_W_classes.pdf}
	\caption{}
	\label{fig:pca_rep}
	\end{subfigure}
	\caption{3D-scatterplot of (a) S2 BOA fixed-length time series (45 observations) and (b) representation, over three principal components obtained by PCA.}
	\label{fcn_losses}
\end{figure}
% \begin{figure}[H]
% 	\centering
% 	\includegraphics[width=0.4\linewidth]{figures/PCA_3D.png}
% 	\caption{Representation over three principal components obtained by PCA applied to the reconstruction difference of the AEs.}
% 	\label{fig:pca}
% \end{figure}
Distance metrics were used to assess the distance between classes. 
Table \ref{distance_metrics_results} presents a comparison of the inter-class separability on the input spectral-temporal data and the generated representations, measured by Silhouette score (SS), Calinski-Harabasz index (CH) and Davies-Bouldin Index (DBI) (see Appendix \ref{app_distance_m} for a description).
In the same way as in the qualitative analysis, distance scores demonstrate much higher separability on the representation space than on the initial data.
\begin{table}[H]
	\centering
	\caption{Class distance assessment of the input S2 optical dataset and the representations produced by the proposed method. $SS$ ranges from -1 for incorrect clustering and +1 for highly dense clustering. $CH$ larger scores indicates better separability. $DBI$ ranges from 0 to $\infty$ and the closer to zero the better partition.}
	\begin{tabular}{c|c|c}
		\hline
		Distance metric & BOA & Our approach \\
		\hline  	
		$SS$ & -0.76 & \textbf{0.18} \\
		$CH$ & 1.4 & \textbf{48678.5} \\
		$DBI$ & 72.44  & \textbf{9.73} \\
		\hline
	\end{tabular}
	\label{distance_metrics_results}
\end{table}

\subsection{Evaluating representations in the classification of crop types}
% Class separability and classification metrics are used in this work to assess the quality of the derived representations.
%Different quality indicators can be considered for representations assessment. For the particular case of representations for classification, class separability and classification metrics are used in this work.
% For evaluating the representations, there are different methods like clustering, distance metrics \cite{Shutaywi2021}, rankme \cite{Garrido2023, Balestriero2023}, . We 
% \cite{Garrido2023}

% \subsubsection{Fully conected network classifier}

% We decided to use a fully conected network as the classification model, 
% where the input are the representations with their correspond label. 
% In this part, it is important to note that the dataset is imbalanced, 
% i.e., all the classes have a different number of samples, which makes the classification model more sensitive to overfitting. 
% The description of the classifier is described in the Table \ref{fcn_hyperparameters}.
After the representations have been learned, a 3-layer FCN was used as classification model, where the inputs are the generated representations and their corresponding labels. The parameters of the classifier are detailed in Table \ref{fcn_hyperparameters}.
The representation values derived by DOY embeddings are removed from our representation vectors, since averaging these values induces redundancy to the classification model. Hence, the input dimensionality is defined by $B$ spectral bands plus 5 spectral indices.
\begin{table}[H]
	\centering
	\caption{Classification model hyperparameters. $B$ denotes the number of bands, and $C$ the number of AEs. Note that only 5 additional features (the spectral vegetation indices) are added to the spectral bands, since DOY features are excluded from the representations in the training of the classification model.}
	\begin{tabular}{c|c}
	\hline
		\multicolumn{2}{c}{Hyperparameters} \\ \hline
		Input size & $(B + 5) \times C = 135$ \\
		Epochs & 100 \\ 
		% Patience & 100 \\ 
		% Min. delta & 1e-5 \\ 
		Batch size & 100\\ 
		Units in hidden layers & 128, 64, 32 \\  
		Learning rate & 1e-5 \\
		Optimizer & Adam \\ 
		Loss & Categorical crossentropy \\ \hline
	\end{tabular}
	\label{fcn_hyperparameters}
\end{table}

% \subsubsection{Confusion matrix}
% In the figure \ref{confusion_matrix} we can see the confusion matrix where, it shows the results of the fcn classifier. 
% In the principal diagonal you can see the samples were classified sucessfull and out of it the samples misclassified.

% \begin{figure}[H]
% 	\includegraphics[width=.7\textwidth]{figures/confusion_matrix_test.pdf}
% 	\caption{Confusion matrix of the FCN prediction for the testing data.}
% 	\label{confusion_matrix}
% \end{figure}


% \subsection{Results}
% In this section, quantitative results of the proposed framework are presented and compared with results reported in \cite{Russwurm2020} for different state of the art methods. 
% For qualitative interpretation, classification maps are shown using the obtained representations.

% \subsubsection{Quantitative results}
In addition to monitoring training and validation loss functions, the overall accuracy (OA) is monitored as indicator of classification model performance. Due to the clear imbalance among classes, BreizhCrops is an extremely challenging dataset. Therefore, the Matthews correlation coefficient (MCC), which is robust to class imbalance, is also monitored for a less biased evaluation. Figure \ref{fcn_losses} presents the progress of these metrics during model fitting.
In Figure \ref{fig:fcn_loss} MAE on validation dataset follows the same descending trend as in the training dataset, indicating that no overfitting occurs during training. From Figures \ref{fig:fcn_accuracy} and \ref{fig:fcn_mcc}, it can be seen that OA and MCC are slightly better on the training dataset.
% The loss function is still decreasing for training but not for validation in Figure \ref{fig:fcn_loss}. 
% The overall accuracy of the Figure \ref{fig:fcn_accuracy} is still increasing to an acceptable level.
% The imbalance in the dataset led to the consideration of the MCC metric (see Figure \ref{fig:fcn_mcc}).
\begin{figure}[H]
	\centering
	\begin{subfigure}[t]{0.3\linewidth}
		\centering
		\includegraphics[width=\linewidth]{figures/classification_results/fcn_loss.pdf}
	  \caption{}
	  \label{fig:fcn_loss}
	\end{subfigure}
	\begin{subfigure}[t]{0.3\linewidth}
		\centering
		\includegraphics[width=\linewidth]{figures/classification_results/fcn_accuracy.pdf}
	\caption{}
	\label{fig:fcn_accuracy}
	\end{subfigure}
	\begin{subfigure}[t]{0.3\linewidth}
		\centering
		\includegraphics[width=\linewidth]{figures/classification_results/fcn_mcc.pdf}
    \caption{}
	\label{fig:fcn_mcc}
	\end{subfigure}
	\caption{FCN training and validation (a) MAE (b) OA and (c) MCC functions over epochs.}
	\label{fcn_losses}
\end{figure}

The confusion matrix shown in Figure \ref{confusion_matrix} presents the model performance on the testing dataset.
While the model performs relatively accurate for wheat, rapeseed and corn samples, permanent and temporary meadows are not separable. This type of missclassification is due to the similarity in nature of both crop types.
Additionally, the small and challenging classes in this dataset, i.e., sunflower, orchards and nuts, are also totally misclassified mainly due to the very limited number of samples.
\begin{figure}[H]
	\centering
	\includegraphics[width=.57\textwidth]{figures/confusion_matrix_fcn_run_9.pdf}
	\caption{Confusion matrix of the FCN prediction for the testing data.}
	\label{confusion_matrix}
\end{figure}
%In the Table \ref{Quantitative_results} we can see the summary results,
Table \ref{Quantitative_results} presents a performance comparison of our method with convolutional, recurrence and attention methods. 
TempCNN, OmniscCNN, LSTM, StarRNN, Transformer, and our proposed method with the generated representations as input to a FCN are evaluated by OA, average accuracy (AA), F1 score, and Cohen's kappa coefficient ($\kappa$). 
Additionally, details on the processor used, the number of parameters, and runtime are presented. 
We present the results reported in \cite{Russwurm2020}, since the code used for those experiments is not publicly available and it is not straightforwardly reproducible. 
Notwithstanding, the same test data points were used in our experiments, hence, results are directly comparable.

The OA achieved with our method is 0.74, while a Transformer model reaches 0.80. However, Transfomer is composed by 188,429 trainable parameters, while ours only needs 6,825, i.e., ~28 times less trainable parametes, which is directly related to the computational load and processing time. 
Moreover, other complex convolutional models, such as TempCNN and OmniscCNN, achieve 0.79 and 0.73 OA, i.e., only 0.05 higher and 0.01 lower than our method respectively, but more 400 times more parameters.
Likewise, Transformer's AA has the highest score 0.58, while our method reached 0.53, slightly below transformers but still within a competitive range.

The F1 score, which balances precision and recall, and therefore provides a balanced assessment of the model, was also the highest in the LSTM and Transformer models with 0.80, while our representations-based FCN achieved 0.74, reflecting its ability to maintain a reasonable balance despite the imbalanced class distribution.
Similarly, LSTM and Transformer models reach the highest Kappa's coefficient (0.75), while our method's is strongly penalized due mostly to the proportion of mismatches on barley, and permanent and temporary meadows. Although our method reaches 0.10 lower score than the competing models, it is still relatively good, especially considering the shallowness of the model.
\begin{table}[H]
	\centering
	\caption{Classification performance evaluation of benchmarked models. All models were evaluated over the same testing dataset.}
	\footnotesize
	\begin{tabular}{c| c c c c c c}
	\hline
	 & TempCNN & OmniscCNN & LSTM            & StartRNN & Transformer     & \textbf{Representations-FCN}\\[1.1ex]
	\hline   
	OA & 0.79    & 0.73      & \textbf{0.80}   & 0.79     & \textbf{0.80}   & \textbf{0.74} \\
	AA & 0.55    & 0.52      & 0.57            & 0.56     & 0.58            &  0.53\\
	F1 & 0.79    & 0.72      & 0.80            & 0.79     & 0.80            &  0.74\\
	k                        & 0.73    & 0.65      & 0.74            & 0.73     & 0.75            &  0.66\\ [1.2ex]
	\hline
	Processor &\multicolumn{5}{c}{\makecell{8X NVIDI Tesla P100 \\ 16GB/GPU 28,672 Total\\ NVIDIA CUDA Cores}} & \makecell{Intel® Core™ \\i5-1035G1 \\CPU @ 1.00GHz\\ $\times$8 Mesa Intel® \\UHD Graphics\\(ICL GT1)}\\ [1.1ex] \hline
	N° param & 3,199,501 & 2,739,737 & 1,339,431     & 72,103   & 188,429         & \textbf{6,825} \\
	\makecell{Runtime in\\ $[it/s]$}  & 1.25 & 1.02      & 1.16           & 1.02     & 1.20              & 0.75 \\
	\hline
\end{tabular}
\label{Quantitative_results}
\end{table}

\subsection{Qualitative results}

%\textcolor{yellow}{\textbf{This part belong to the section methodology (CHECK!)}}
%In this section we use 
%Google Earth Engine (GEE) was used for download a multitemporal multispectral image of Sentinel 2 sensor in the FRH04 region of the 2017 year (see Figure \ref{fig:gee_img}) in order to obtain classification maps. \\
As BreizhCrops data are sparse geographic points and not images, a qualitative assessment based on classification maps cannot be directly performed on this dataset. 
Nevertheless, to enable a qualitative analysis, 67 Sentinel 2 multispectral images from 2017 of a subregion in FRH04 (test region) were downloaded and preprocessed. 
A representative area was defined drawing a polygon where most of the classes (barley, wheat, corn, rapeseed, temporary meadows and permanent meadows) are present (Figure \ref{fig:gee_img}). 
% As for the BreizCrops dataset, only 10m and 20m resolution bands (resampled to 10m) were used. 
% Besides, temporal embeddings described in subsection \ref{data_preprocessing}, as well as spectral indices mentioned in subsection \ref{dataset} are added as features.
% \begin{figure}[H]
% 	\centering
% 	\includegraphics[height=0.33\linewidth,width=0.3\linewidth]{figures/reconstruction_errors/test_aoi_RGB.pdf}
% 	\caption{True color image of the study area in 2017.}
% 	\label{fig:gee_img}
% \end{figure}

%The first step was to draw a polygon in this region where there are the majority of the classes, after this we download all images considering only 10 bands (10 and 20 meters of resolution). The second step was to do a preprocessing where was necessary to add the spectral indices and the day of the year in each image. After this we obtained the representations of this multitemporal multispectral image using the AEs trained previously. In the Figure \ref{representations} we can see the feature second of the representations of each AE.
Representations for this study area are produced by passing individual pixels from the imagery dataset through the inference workflow outlined in Figure \ref{Inference}. 
The false color images generated by combining three random representations bands presented in Figures \ref{fig:color_map_a}, \ref{fig:color_map_b} and \ref{fig:color_map_c}, as well as individual representations shown in Figure \ref{representations}, contrast the crop fields in the new representations space.
\begin{figure}[H]
	\centering
	\begin{subfigure}[t]{0.23\linewidth}
		\centering
		\includegraphics[height=\linewidth,width=0.95\linewidth]{figures/reconstruction_errors/test_aoi_RGB.pdf}
	\caption{}
	\label{fig:gee_img}
	\end{subfigure}
	\begin{subfigure}[t]{0.23\linewidth}
		\centering
		\includegraphics[height=\linewidth,width=0.95\linewidth]{figures/combinations/combinations_[59, 84, 81].pdf}
	\caption{}
	\label{fig:color_map_a}
	\end{subfigure}
	\begin{subfigure}[t]{0.23\linewidth}
		\centering
	  \includegraphics[height=\linewidth,width=0.95\linewidth]{figures/combinations/combinations_[30, 11, 141].pdf}
	\caption{}
	\label{fig:color_map_b}
	\end{subfigure}
	\begin{subfigure}[t]{0.23\linewidth}
		\centering
		\includegraphics[height=\linewidth,width=0.95\linewidth]{figures/combinations/combinations_[45, 66, 57].pdf}
    \caption{}
	\label{fig:color_map_c}
	\end{subfigure}
	\caption{(a) True color image of the study area in 2017 and false color images combining three random representations per map (b) 59-84-81, (c) 30-11-141, and (d) 45-66-57.}
	\label{color_maps}
\end{figure}

\begin{figure}[H]
	\centering
	\begin{subfigure}[t]{0.23\linewidth}
		\centering
		\includegraphics[height=\linewidth,width=0.95\linewidth]{figures/reconstruction_errors/reconstruction_B2_AEs0.pdf}
	\caption{}
	\label{fig:r_aes_b2_0}
	\end{subfigure}
	\begin{subfigure}[t]{0.23\linewidth}
		\centering
	  \includegraphics[height=\linewidth,width=0.95\linewidth]{figures/reconstruction_errors/reconstruction_B2_AEs1.pdf}
	\caption{}
	\label{fig:r_aes_b2_1}
	\end{subfigure}
	\begin{subfigure}[t]{0.23\linewidth}
		\centering
		\includegraphics[height=\linewidth,width=0.95\linewidth]{figures/reconstruction_errors/reconstruction_B2_AEs2.pdf}
    \caption{}
	\label{fig:r_aes_b2_2}
	\end{subfigure}
	\begin{subfigure}[t]{0.23\linewidth}
		\centering
		\includegraphics[height=\linewidth,width=0.95\linewidth]{figures/reconstruction_errors/reconstruction_B2_AEs3.pdf}
    \caption{}
	\label{fig:r_aes_b2_3}
	\end{subfigure}\\[0.2cm]
	\begin{subfigure}[t]{0.23\linewidth}
		\centering
		\includegraphics[height=\linewidth,width=0.95\linewidth]{figures/reconstruction_errors/reconstruction_B2_AEs4.pdf}
	 \caption{}
	\label{fig:r_aes_b2_4}
	\end{subfigure}
	\begin{subfigure}[t]{0.23\linewidth}
		\centering
		\includegraphics[height=\linewidth,width=0.95\linewidth]{figures/reconstruction_errors/reconstruction_B2_AEs5.pdf}
    \caption{}
	\label{fig:r_aes_b2_5}
	\end{subfigure}
	\begin{subfigure}[t]{0.23\linewidth}
		\centering
		\includegraphics[height=\linewidth,width=0.95\linewidth]{figures/reconstruction_errors/reconstruction_B2_AEs6.pdf}
	\caption{}
	\label{fig:r_aes_b2_6}
	\end{subfigure}
	\begin{subfigure}[t]{0.23\linewidth}
		\centering
		\includegraphics[height=\linewidth,width=0.95\linewidth]{figures/reconstruction_errors/reconstruction_B2_AEs7.pdf}
	\caption{}
	\label{fig:r_aes_b2_7}
	\end{subfigure}
	\caption{Visualization of a subset of the representation bands produced by our RL AE-based framework. We show here only 8 out of $9 \times 15$ available representations.}
	\label{representations}
\end{figure}

% \begin{figure}[H]
% 	\centering
% 	\begin{subfigure}[t]{0.3\linewidth}
% 		\centering
% 		\includegraphics[width=0.8\linewidth]{figures/combinations/combinations_[59, 84, 81].pdf}
% 	\caption{}
% 	\label{fig:color_map_a}
% 	\end{subfigure}
% 	\begin{subfigure}[t]{0.3\linewidth}
% 		\centering
% 	  \includegraphics[width=0.8\linewidth]{figures/combinations/combinations_[30, 11, 141].pdf}
% 	\caption{}
% 	\label{fig:color_map_b}
% 	\end{subfigure}
% 	\begin{subfigure}[t]{0.3\linewidth}
% 		\centering
% 		\includegraphics[width=0.8\linewidth]{figures/combinations/combinations_[45, 66, 57].pdf}
%     \caption{}
% 	\label{fig:color_map_c}
% 	\end{subfigure}
% 	\caption{Color maps of the representations with band combinations (a) 59,84,81, (b) 30,11,141 and (c) 45,66,57.}
% 	\label{color_maps}
% \end{figure}

The classification map produced by the trained classification FCN, with the representations as input data, is presented in Figure \ref{fig:maps} together with the ground truth.
% The corresponding ground truth is shown in Figure \ref{fig:frh04_labels}. 
% We can mostly observe classification errors in classes, such as barley, wheat and rapeesed, since the similarity in the spectral-temporal characteristics is narrow. %\textcolor{orange}{\textbf{By increasing the spectral and temporal resolution, the overlap can potentially be reduced}}.\\
While the spatial distribution of some classes seems well captured, a number of classes are only poorly mapped.
%\textcolor{orange}{we only have the half of temporal observations}
\begin{figure}[H]
	\centering
	\begin{subfigure}[t]{0.3\linewidth}
		\centering
		\includegraphics[height=\linewidth,width=0.95\linewidth]{figures/frh04_clip.png}
	\caption{}
	\label{fig:frh04_labels}
	\end{subfigure}\hspace{1cm}
	% \begin{subfigure}[t]{0.3\linewidth}
	%   \includegraphics[width=\linewidth]{figures/classification_clip.png}
	% \caption{}
	% \label{fig:classification_map}
	% \end{subfigure}
	\begin{subfigure}[t]{0.5\linewidth}
		\centering
		\includegraphics[height=0.6\linewidth,width=0.85\linewidth]{figures/classification_2017_2018_with_labels_2.pdf}
	  \caption{}
	  \label{fig:classification_map}
	\end{subfigure}
	\caption{(a) Study area ground truth at field level (polygons) and (b) representations-based FCN pixel-wise classification (raster).}
	\label{fig:maps}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}
Our AEs-based methodology for representation learning (RL) addresses the problem of cloud-corrupted optical data by mapping RS spectral-temporal features into informative, and gap free representations.
Our spectral and temporal-based approach produces pixel level comprehensive representations, avoiding the need to employ complex spatial-based classifiers.
% This research presents specifically crop type classification task-guided representations, however, the extrapolation to other classification task is straightforward.

The method proposed in this paper has as main advantage its ability to produce pixel-wise representations independently of the number of cloud free samples.
Therefore, complex interpolation/gap filling methods, used in other approaches, are not needed.
Other solutions, such as obtaining fixed-length time series matching with the input size of a neural network are not needed.
However, as other methods, our approach is still negatively affected when limited cloud free temporal observations are available.

% The dataset is imbalanced, making it challenging to achieve accurate results. 
% We used the same regions for training, validation, and testing as in \cite{Russwurm2020}.

Despite the shallowness of the neural networks used for both, representations learning as well as classification, our method performs satisfactory extracting meaningful information for downstream crop classification. 
While other deep classification networks are capable of achieving higher OA scores, our light-weight model performs with similar quality.

Models such as TempCNN, OmniscCNN, LSTM, StarRNN, and Transformer, need to be executed on powerful equipment well-suited for handling complex models. In contrast, our full framework was easily launched on a significantly less powerful CPU. This showcases our method's efficiency and adaptability to lower-end hardware and/or scalability to large geographic areas.
In terms of number of trainable parameters, convolutional and recurrent models require millions of parameters, which indicates their high computational demands. A shallow three layer FCN, such as the one tested in this paper, is significantly less computationally complex.

% The runtime (measured in iterations per second, lt/s) is another critical factor. FCN stands out with the fastest runtime of 0.75 lt/s, demonstrating its efficiency in processing data quickly. In comparison, more complex models like TempCNN and LSTM have runtimes of 1.25 lt/s and 1.16 lt/s, respectively, reflecting the trade-off between complexity and speed.

% For evaluating the representations using a Fully Convolutional Network (FCN), to determine whether they were sufficiently representative for the classifier to accurately distinguish between different crops.

% In Figure \ref{confusion_matrix}, we can observe the samples that were successfully classified in each class, as well as the classes where the model misclassified samples. 
The dataset used in the experiments of this work is particularly challenging, as sunflower, orchards, and nuts were not separable, mainly due to the limited number of labeled samples, which restricts our model from learning enough informative and significative representations before classification.
However, there are no computational or data limitations to apply our approach on different areas and with datasets from other optical sensors, such as Landsat, and even from radar sensors.
Although, this research presents specifically crop type classification task-guided representations, the extrapolation to other classification task is straightforward.

% For the qualitative analysis, we utilize Sentinel-2 imagery from the same geographic region, which we access via the Google Earth Engine tool. This imagery allows us to generate detailed classification maps. Figure \ref{representations} illustrates the key differences between the various representations produced by AEs (AEs). These differences are crucial for understanding how each AE model generating the representations. Based on these representations, we can create color maps, as shown in Figure \ref{color_maps}. These maps display how different crop types are assigned similar colors, enabling us distinguishing between crop types.

% Our analysis involved evaluating the representations generated by the AEs (AEs) using a Fully Convolutional Network (FCN) to produce a classification map. The structure of the FCN model used for this evaluation is consistent with the classification model described in Table \ref{fcn_hyperparameters}. The classification task aimed to distinguish between six crop classes: barley, wheat, corn, rapeseed, temporary meadows, and permanent meadows.

% The corresponding ground truth for this classification task is depicted in Figure \ref{frh04_labels}. This ground truth serves as a benchmark against which the performance of the FCN can be assessed. Figure \ref{fig:classification_map} shows the classification map generated by the FCN based on the representations obtained from the AEs.

% For examining the results, it is evident that the FCN successfully classified most of the crops. However, certain crops, particularly barley, wheat, and rapeseed, were frequently misclassified. This misclassification can be attributed to the similar spectral characteristics shared by these crops. Since Sentinel-2 images rely on spectral data to differentiate between land cover types, the subtle spectral differences between these crops can be challenging to discern, leading to overlap in classification.

% In summary, the use of AEs for crop type classification achieve acceptable score maintaining the low load computational to comparative with the convolutional, recurrence and attention methods. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}
Based on the results reported in this paper, we derive the following conclusions:
\begin{enumerate}
	\item Qualitative evaluation based on distance metrics demonstrate that the representations produced by our method accomplished the objective of mapping RS spectral-temporal data to a feature space where inter-class separability is higher than in the initial Sentinel 2 BOA time series.
	\item Classification scores obtained by our method, in combination with the comparison of number of trainable parameters and execution time, demonstrate that our method, althought it is not the best classification, reaches competitve accuracy with much less computational load. Therefore, scalability to larger areas is feasible and not excesively time consuming.
	% \item The FCN model, while simpler and operating on less powerful hardware, delivers a strong performance across all metrics. It achieves a respectable OA of 0.74, an AA of 0.53, and an F1 score of 0.74, which are competitive, especially considering its minimal parameter count and fast runtime. This makes FCN a highly efficient and practical choice, particularly in scenarios where computational resources are limited. While more complex models like LSTM and Transformer perform better overall, FCN's efficiency and adaptability make it a viable alternative, balancing accuracy and resource usage effectively.
	\item The confusion matrix, as well as the classification map provide valuable insights that our method in fact performs correctly on the majority of classes evaluated in this work, especially for those with sufficient training samples. However, the challenge of distinguishing between crops with similar spectral characteristics remains. Addressing this issue will be essential for improving the robustness and precision of crop classification models in future studies and will most probably involve the concurrent use of additional sensor modalities.
\end{enumerate}

\section{Open Issues}
Outside the scope of this work, there are still some points to consider in future research:
\begin{itemize}
	\item Implementation of a fully unsupervised methodology for training autoencoders without relying on labeled data.
	\item Evaluation of the proposed methodolgy on other optical sensors, radar sensors or combination of both data sources.
	\item Fine-tuning the classification model to find a better balance between performance metrics and number of trainable parameters.
\end{itemize}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Patents}

% This section is not mandatory, but may be added if there are patents resulting from the work reported in this manuscript.

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \vspace{6pt} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% optional
%\supplementary{The following supporting information can be downloaded at:  \linksupplementary{s1}, Figure S1: title; Table S1: title; Video S1: title.}

% Only for journal Methods and Protocols:
% If you wish to submit a video article, please do so with any other supplementary material.
% \supplementary{The following supporting information can be downloaded at: \linksupplementary{s1}, Figure S1: title; Table S1: title; Video S1: title. A supporting video article is available at doi: link.}

% Only for journal Hardware:
% If you wish to submit a video article, please do so with any other supplementary material.
% \supplementary{The following supporting information can be downloaded at: \linksupplementary{s1}, Figure S1: title; Table S1: title; Video S1: title.\vspace{6pt}\\
%\begin{tabularx}{\textwidth}{lll}
%\toprule
%\textbf{Name} & \textbf{Type} & \textbf{Description} \\
%\midrule
%S1 & Python script (.py) & Script of python source code used in XX \\
%S2 & Text (.txt) & Script of modelling code used to make Figure X \\
%S3 & Text (.txt) & Raw data from experiment X \\
%S4 & Video (.mp4) & Video demonstrating the hardware in use \\
%... & ... & ... \\
%\bottomrule
%\end{tabularx}
%}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\authorcontributions{ Conceptualization, A.G. and C.A.; methodology, C.A., A.G. and J.L.; software, A.G. and J.L.; writing—original draft preparation, A.G. and J.L.; writing—review and editing, C.A. and D.T.; visualization, A.G.; supervision, C.A. and D.T.; project administration, A.G.; funding acquisition, D.T.}

\funding{ This research was funded by CONAHCYT grant number 1001207.}%Please add: ``This research received no external funding'' or ``
% '' and  and ``The APC was funded by XXX''. Check carefully that the details given are accurate and use the standard spelling of funding agency names at \url{https://search.crossref.org/funding}, any errors may affect your future funding.}

% \institutionalreview{}%In this section, you should add the Institutional Review Board Statement and approval number, if relevant to your study. You might choose to exclude this statement if the study did not require ethical approval. Please note that the Editorial Office might ask you for further information. Please add “The study was conducted in accordance with the Declaration of Helsinki, and approved by the Institutional Review Board (or Ethics Committee) of NAME OF INSTITUTE (protocol code XXX and date of approval).” for studies involving humans. OR “The animal study protocol was approved by the Institutional Review Board (or Ethics Committee) of NAME OF INSTITUTE (protocol code XXX and date of approval).” for studies involving animals. OR “Ethical review and approval were waived for this study due to REASON (please provide a detailed justification).” OR “Not applicable” for studies not involving humans or animals.}

% \informedconsent{}

% \dataavailability{}

% Only for journal Nursing Reports
%\publicinvolvement{Please describe how the public (patients, consumers, carers) were involved in the research. Consider reporting against the GRIPP2 (Guidance for Reporting Involvement of Patients and the Public) checklist. If the public were not involved in any aspect of the research add: ``No public involvement in any aspect of this research''.}

% Only for journal Nursing Reports
%\guidelinesstandards{Please add a statement indicating which reporting guideline was used when drafting the report. For example, ``This manuscript was drafted against the XXX (the full name of reporting guidelines and citation) for XXX (type of research) research''. A complete list of reporting guidelines can be accessed via the equator network: \url{https://www.equator-network.org/}.}

% \acknowledgments{}

\conflictsofinterest{ The authors declare no conflict of interest} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Optional
% \sampleavailability{}%Samples of the compounds ... are available from the authors.}

%% Only for journal Encyclopedia
%\entrylink{The Link to this entry published on the encyclopedia platform.}

\abbreviations{Abbreviations}{
The following abbreviations are used in this manuscript:\\

\noindent 
\begin{tabular}{@{}ll}
AA & Average accuracy\\
AEs & Autoencoders\\
ANN & Artificial neural networks\\
BOA & Bottom of atmosphere\\
CH & Calinski Harabasz\\
DBI & Davies Bouldin Index\\
DOY & Day of the year\\
DT & Decission trees\\
FCN & Fully connected network\\
GEE & Google earth engine\\
$\kappa$ & Cohen's kappa\\
LSTM &  Long-short term memory\\
MAE & Mean absolut error\\
MCC & Matthews correlation coefficient\\
ML & Machine learning\\
OA & Overall accuracy\\
PCA & Principal component analysis\\
RF & Random forest\\
RL & Representation learning\\
ROI & Region of interest\\
RS & Remote sensing\\
SS & Silhouette score\\
STBT & Spectral-temporal Barlow twins\\
SVM & Support vector machine\\
TOA & Top of atmosphere\\
UA & User's accuracy\\
% CSF & Contrastive Sensor Fusion\\
% S2 & Sentinel 2\\
% PA & Producers Accuracy\\
% NDWI & Normalized Difference Water Index\\
% NDVI & Normalized Difference Vegetation Index\\
% NDTI & Normalized Difference Tillage Index\\
% NDSVI & Normalized Difference of Senescent Vegetation ISndex\\
% EVI & Enhanced Vegetation Index\\
\end{tabular}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Optional
\appendixtitles{no} % Leave argument "no" if all appendix headings stay EMPTY (then no dot is printed after "Appendix A"). If the appendix sections contain a heading then change the argument to "yes".
\appendixstart
\appendix
\section[\appendixname~\thesection]{Hyperparameters random search}\label{app_a}
AEs hyperparameters were defined after an extensive random search. One hundred configurations with four variable hyperparameters were launched and evaluated with three classification and three distance metrics.
The search spaces for each hyperparameter are:
\begin{itemize}
	\item Units: $U\left\{1,16\right\}$
	\item Batch size rate: $U\left[0.1,0.3\right]$ 
	\item Learning rate: $U\left[1\times10^{-3},9\times10^{-6}\right]$
	\item Loss: $\left\{0,1\right\}$
\end{itemize}
where $U\left\{\cdot\right\}$ and $U\left[\cdot\right]$ denote uniform discrete and continuous distribution respectively.
Final configuration reported in Table \ref{hyperparameters_aes} was defined according to the pairwise correlation between hyperparameters and metrics presented in Figure \ref{correlation_matrix}.
\begin{figure}[H]
	\centering
	\includegraphics[width=.67\textwidth]{figures/hyperparameter_results_update_latex.pdf}
	\caption{Hyperparameters and quality indicators correlation matrix.}
	\label{correlation_matrix}
\end{figure}
\section[\appendixname~\thesection]{Separability metrics}\label{app_distance_m} 
These metrics quantify how separable a set of classes/clusters are from each other.
\textbf{Silhouette score}:
    \begin{equation}
        SS = \frac{b-a}{max(a,b)}
    \end{equation}
where $a$ is the mean distance between a sample and all other points in the same class, $b$ is the mean distance between a sample and all other points in the next nearest cluster. The score is bounded between -1 for incorrect clustering and +1 for highly dense clustering.\\

\textbf{Calinski-Harabasz Index}
    \begin{equation}
        CH=\frac{\left[\frac{\sum_{k=1}^{K} n_{k}\left\|c_{k}-c\right\|^{2}}{K-1}\right]}{\left[\frac{\sum_{k=1}^{K} \sum_{i=1}^{n_{k}}\left\|d_{i}-c_{k}\right\|^{2}}{N-K}\right]}
    \end{equation}
    where $d_i$ is the feature vector of data point $i$, $n_k$ is the size of the $k^{th}$ cluster, $c_k$ is the feature vector of the centroid of the $k^{th}$ cluster, $c$ is the feature vector of the global centroid of the entire dataset, $N$ is the total number of data points. The higher the score is the better separation.\\

\textbf{Davies-Bouldin Index}
    \begin{equation}
        R_{ij} = \frac{s_{i} + s_{j}}{d_{ij}}
    \end{equation}
    \begin{equation}
        DBI = \frac{1}{k} \sum_{i=1}^{k} max_{i \neq j} R_{ij}
    \end{equation}
    where $s_{i}$ is the average distance between each point of cluser $i$ and the centroid of that cluser, $d_{ij}$ is the distance between cluser centroids $i$ and $j$. The score is between $0$ and $\infty$, and the values closer to zero indicate a better partition.\\

\section[\appendixname~\thesection]{Classification metrics}\label{distance_m} 
For evaluating the predictions obtained to the FCN, we consider to compute the same metrics that the authors in \cite{Russwurm2020} used for comparative purposes of this work. We compute through of the confussion matrix the equations shown follow:\\

Given a confusion matrix $\mathbf{M} \in \mathbb{R}^{C \times C}$ where $C$ is the number of classes, the $OA$ is computed with the equation \ref{eqn:OA}.
\begin{equation}
	\label{eqn:OA}
	OA = \frac{\sum_{i=1}^{C} \mathbf{M}_{ii}}{\sum_{i=1}^{C} \sum_{j=1}^{C} \mathbf{M}_{ij}}
\end{equation}

From $\mathbf{M}$ to a class-wise confusion matrix following the approach one versus all, the producers accuracy also known as precision is computed by \\
\begin{equation}
	\label{eqn:PA_c}
	PA_c = \frac{TP_c}{TP_c + FP_c}
\end{equation}
where $TP_c$ is the true positive and $FP_c$ is the false positive of the class $c$. \\
Then, the AA is computed as follows
\begin{equation}
	\label{eqn:AA}
	AA = \frac{\sum_{c=1}^{C} PA_c}{C}
\end{equation}
The user's accuracy ($UA_c$) also known as recall is compute as follows
\begin{equation}
	\label{eqn:UA_c}
	UA_c = \frac{TP_c}{TP_c + FN_c}
\end{equation}
where $TP_c$ is the true positive and $FN_c$ is the false negative of the class $c$.\\
With the equation \ref{eqn:PA_c} and \ref{eqn:UA_c} we can compute the Weighted F1-score per class ($F1_c$) as follows:
\begin{equation}
	\label{eqn:F1_c}
	F1_c = 2 \frac{PA_c \times UA_c}{PA_c + UA_c}
\end{equation}
The formula for Cohen's kappa ($k$) is the probability of agreement minus the probability of random agreement, divided by one minus the probability of random agreement.\\
\begin{equation}
	\label{eqn:k}
	k= \frac{p_o - p_e}{1 - p_e}
\end{equation}
where $p_o$ is is the relative observed agreement among raters, and $p_e$ is the hypothetical probability of chance agreement.

% \subsection[\appendixname~\thesubsection]{}

% \begin{table}[H] 
% \caption{This is a table caption.\label{tab5}}
% \newcolumntype{C}{>{\centering\arraybackslash}X}
% \begin{tabularx}{\textwidth}{CCC}
% \toprule
% \textbf{Title 1}	& \textbf{Title 2}	& \textbf{Title 3}\\
% \midrule
% Entry 1		& Data			& Data\\
% Entry 2		& Data			& Data\\
% \bottomrule
% \end{tabularx}
% \end{table}

% \section[\appendixname~\thesection]{}
% All appendix sections must be cited in the main text. In the appendices, Figures, Tables, etc. should be labeled, starting with ``A''---e.g., Figure A1, Figure A2, etc.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{adjustwidth}{-\extralength}{0cm}
%\printendnotes[custom] % Un-comment to print a list of endnotes

\reftitle{References}

% Please provide either the correct journal abbreviation (e.g. according to the “List of Title Word Abbreviations” http://www.issn.org/services/online-services/access-to-the-ltwa/) or the full name of the journal.
% Citations and References in Supplementary files are permitted provided that they also appear in the reference list here. 

%=====================================
% References, variant A: external bibliography
%=====================================
\bibliography{references}

%=====================================
% References, variant B: internal bibliography
%=====================================
% \begin{thebibliography}{999}
% % Reference 1
% \bibitem[Author1(year)]{ref-journal}
% Author~1, T. The title of the cited article. {\em Journal Abbreviation} {\bf 2008}, {\em 10}, 142--149.
% % Reference 2
% \bibitem[Author2(year)]{ref-book1}
% Author~2, L. The title of the cited contribution. In {\em The Book Title}; Editor 1, F., Editor 2, A., Eds.; Publishing House: City, Country, 2007; pp. 32--58.
% % Reference 3
% \bibitem[Author3(year)]{ref-book2}
% Author 1, A.; Author 2, B. \textit{Book Title}, 3rd ed.; Publisher: Publisher Location, Country, 2008; pp. 154--196.
% % Reference 4
% \bibitem[Author4(year)]{ref-unpublish}
% Author 1, A.B.; Author 2, C. Title of Unpublished Work. \textit{Abbreviated Journal Name} year, \textit{phrase indicating stage of publication (submitted; accepted; in press)}.
% % Reference 5
% \bibitem[Author5(year)]{ref-communication}
% Author 1, A.B. (University, City, State, Country); Author 2, C. (Institute, City, State, Country). Personal communication, 2012.
% % Reference 6
% \bibitem[Author6(year)]{ref-proceeding}
% Author 1, A.B.; Author 2, C.D.; Author 3, E.F. Title of presentation. In Proceedings of the Name of the Conference, Location of Conference, Country, Date of Conference (Day Month Year); Abstract Number (optional), Pagination (optional).
% % Reference 7
% \bibitem[Author7(year)]{ref-thesis}
% Author 1, A.B. Title of Thesis. Level of Thesis, Degree-Granting University, Location of University, Date of Completion.
% % Reference 8
% \bibitem[Author8(year)]{ref-url}
% Title of Site. Available online: URL (accessed on Day Month Year).
% \end{thebibliography}

% If authors have biography, please use the format below
%\section*{Short Biography of Authors}
%\bio
%{\raisebox{-0.35cm}{\includegraphics[width=3.5cm,height=5.3cm,clip,keepaspectratio]{Definitions/author1.pdf}}}
%{\textbf{Firstname Lastname} Biography of first author}
%
%\bio
%{\raisebox{-0.35cm}{\includegraphics[width=3.5cm,height=5.3cm,clip,keepaspectratio]{Definitions/author2.jpg}}}
%{\textbf{Firstname Lastname} Biography of second author}

% For the MDPI journals use author-date citation, please follow the formatting guidelines on http://www.mdpi.com/authors/references
% To cite two works by the same author: \citeauthor{ref-journal-1a} (\citeyear{ref-journal-1a}, \citeyear{ref-journal-1b}). This produces: Whittaker (1967, 1975)
% To cite two works by the same author with specific pages: \citeauthor{ref-journal-3a} (\citeyear{ref-journal-3a}, p. 328; \citeyear{ref-journal-3b}, p.475). This produces: Wong (1999, p. 328; 2000, p. 475)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% for journal Sci
%\reviewreports{\\
%Reviewer 1 comments and authors’ response\\
%Reviewer 2 comments and authors’ response\\
%Reviewer 3 comments and authors’ response
%}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\PublishersNote{}
\end{adjustwidth}
\end{document}

